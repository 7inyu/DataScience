{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.2 Linear Regression\n",
    "----\n",
    "Linear regression has the form\n",
    "$$\n",
    "f(\\mathbf{X}) = \\beta_0 + \\sum_{j=1}^p\\mathbf{X}_j\\beta_j = E[\\mathbf{Y}|\\mathbf{X}]\n",
    "$$\n",
    "where linear means linear in the coefficients.\n",
    "\n",
    "The residual sum of square has the form\n",
    "\n",
    "$$\n",
    "RSS(\\beta) = (y-\\mathbf{X}\\beta)^T(y-\\mathbf{X}\\beta) \n",
    "$$\n",
    "To obtain $\\hat{\\beta}$, we calculate first order derivative and set it to zero:\n",
    "$$\n",
    "\\frac{\\partial RSS}{\\partial \\beta} = -2\\mathbf{X}^T(y-\\mathbf{X}\\beta) = 0\n",
    "$$\n",
    "If we want to obtain an unique solution, we also require the second order differentiation to be positive definite.\n",
    "$$\n",
    "\\frac{\\partial^2RSS}{\\partial\\beta\\partial\\beta^T} = 2\\mathbf{X}^T\\mathbf{X}\n",
    "$$\n",
    "This is equivalent to say $\\mathbf{X}^T\\mathbf{X}$ has full rank, or what we have always talked about: no collinearity between variables.\n",
    "\n",
    "Finally, the unique solution is $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "As a result, the prediction is\n",
    "$$\n",
    "\\hat{\\mathbf{y}}  = \\mathbf{X}\\hat{\\beta} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "we call $\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ the projection matrix. Since we are projecting $\\mathbf{y}$ to the column space of $\\mathbf{X}$.\n",
    "\n",
    "3.2.2 The Gauss-MArkov Theorem\n",
    "---\n",
    "This theorem basically states that for all **linear unbiased estimator**, least square estimates have the smallest variance. But often times, we may pursue biased estimator, since it will reduce variance. This is because of the famous bias-variance trade off:\n",
    "\n",
    "\\begin{align}\n",
    "MSE(\\hat{\\theta}) & = E(\\hat{\\theta} - \\theta)^2 \\\\ \n",
    "& = E(\\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) -\\theta)^2 \\\\\n",
    "& = E(\\hat{\\theta} - E(\\hat{\\theta}))^2 + 2E((\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta}) -\\theta))+E(E(\\hat{\\theta}) -\\theta)^2) \\\\\n",
    "& = Var(\\hat{\\theta}) + E(\\hat{\\theta}) -\\theta)^2\n",
    "\\end{align}\n",
    "\n",
    "where the first term is the variance of estimator and the second bias.\n",
    "\n",
    "3.2.3 Multiple Regression\n",
    "---\n",
    "The way to calculate multiple regression is as follows:\n",
    "1. Initialize $\\mathbf{z}_0 = \\mathbf{x}_0 = \\mathbf{1}$\n",
    "2. For $j=1,2...,p$\n",
    "     \n",
    "     Regress $\\mathbf{x}_j$ on $\\mathbf{z}_0, \\mathbf{z}_1, \\cdots, \\mathbf{z}_{j-1}$ to produce coefficients $\\hat{\\gamma_{lj}} = <\\mathbf{z}_l, \\mathbf{x}_j>/<\\mathbf{z}_l, \\mathbf{z}_l>, l = 0, \\cdots, j-1$ and residual vector $\\mathbf{z}_j = \\mathbf{x}_j - \\sum_{k=0}^{j-1}\\hat{\\gamma_{kj}}\\mathbf{z}_j$\n",
    "3. Regress $\\mathbf{y}$ on the residual $\\mathbf{z}_p$ to give the estimate $\\hat{\\beta}_p$.\n",
    "\n",
    "Essentially, this is saying, we want $\\hat{\\beta}_j$ to represents the additional contribution of $\\mathbf{x}_j$ on $\\mathbf{y}$, when $\\mathbf{x}_j$ has been adjusted for $\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{j-1}$.\n",
    "\n",
    "The above algorithm gives rise to the decomposition $\\mathbf{X} = \\mathbf{Z}\\mathbf{\\Gamma}$, where $\\mathbf{Z}$ has columns $\\mathbf{z}_j$ and $\\mathbf{\\Gamma}$ is upper triangular with entries $\\hat{\\gamma_{lj}}$ since $\\hat{\\gamma_{lj}}$ only exists for $l\\leq j$\n",
    "\n",
    "Let $\\mathbf{D}$ be a diagonal matrix with jth diagonal entry $D_{jj}$ = $||\\mathbf{z}_j||$. Then $\\mathbf{Z}\\mathbf{D}^{-1}$ is really a normalization of $\\mathbf{Z}$.\n",
    "\n",
    "This leads to the famous $\\mathbf{QR}$ decomposition:\n",
    "$$\n",
    "X = \\mathbf{Z}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{\\Gamma} = \\mathbf{QR}\n",
    "$$\n",
    "\n",
    "3.4.1 Ridge Regression\n",
    "---\n",
    "Pay attention to the following:\n",
    "1. Ridge is not invariant under scaling, so make sure nomalize the input before modeling.\n",
    "2. Do data centering first. Since the coefficients shrinkage implies if we shift all $\\mathbf{y}_i$ by c, the prediction may not shift by c. \n",
    "\n",
    "The solution to ridge problem is $\\hat{\\beta}^{ridge} = (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}\\mathbf{y}$, where $\\lambda\\mathbf{I}$ is added so that the problem is nonsingular.\n",
    "\n",
    "Through SVD, we may obtain that\n",
    "$$\n",
    "\\mathbf{X}\\hat{\\beta}^{ridge} = \\sum_{j=1}^{p}\\mathbf{u}_j\\frac{d_j^2}{d_j^2+\\lambda}\\mathbf{u}_j^T\\mathbf{y}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{X=UDV^T}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\mathbf{X^TX = VD^2V^T}\n",
    "$$\n",
    "which is the eigen decomposition of $X^TX$. Since V is a p by p orthogonal matrices. We can see that ridge regression is really shrinking the most on the less important components, namely the ones with smaller eigen values.\n",
    "\n",
    "3.4.3 Least Angle Regression\n",
    "---\n",
    "At $k_{th}$ step, we move the active set in the direction:\n",
    "$$\n",
    "\\delta_k = (\\mathbf{X}^T_{A_k}\\mathbf{X}_{A_k})^{-1}\\mathbf{X}^T_{A_k}\\mathbf{r}_k\n",
    "$$\n",
    "where $\\mathbf{r}_k = y - \\mathbf{X}_{A_k}\\beta_{A_k}$.\n",
    "\n",
    "If you remember what we learnt in multiple regression, this direction is really the solution to the linear regression of $\\mathbf{X}_{A_k}$ on $\\mathbf{r}_k$. Using a simple three dimensional digram, we can easily show that if we increase in this direction, it will indeed keep the correlation tied and decreasing.\n",
    "\n",
    "Comparison between Lasso and Least Angel Regression\n",
    "---\n",
    "Suppose at step j, the active set is $\\mathbf{A}$. Since every $\\mathbf{x}$ in A has the same correlation with the residual, we have:\n",
    "$$\n",
    "\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta) = \\gamma\\mathbf{s}_j, \\forall j \\in \\mathbf{A}\n",
    "$$\n",
    "where $\\gamma$ is the shared correlation and $\\mathbf{s}_j$ is the sign of the inner product.\n",
    "Also, we have $|\\mathbf{x}_k^T(\\mathbf{y}-\\mathbf{X}\\beta)| \\leq \\gamma, \\forall k \\notin \\mathbf{A}$.\n",
    "\n",
    "Now let's look at LASSO, where the objective function is\n",
    "$$\n",
    "R(\\beta) = \\frac{1}{2}||\\mathbf{y}-\\mathbf{X}\\beta||_2^2+\\lambda||\\beta||_1\n",
    "$$\n",
    "\n",
    "Assume that $\\mathbf{B}$ is the active set of variables in the solution for given $\\lambda$. Since this is the optimal solution,and non-differentiable, we need something called Karush-Kuhn_Tucker optimality condistions. According to the condistions, we have\n",
    "$$\n",
    "\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta) = \\lambda sign(\\beta_j), \\forall j \\in \\mathbf{B}\n",
    "$$\n",
    "\n",
    "This shows that LAR and LASSO is almost exactly the same except when the coefficient passes through zero. In this case, the criteria for LASSO is violated and the variable needs to be kicked out.\n",
    "\n",
    "And $|\\mathbf{x}_k^T(\\mathbf{y}-\\mathbf{X}\\beta)| \\leq \\lambda, \\forall k \\notin \\mathbf{B}$, also follows from Karush-Kuhn-Tucker optimality conditions.\n",
    "\n",
    "Karush-Kuhn_Tucker optimality conditions\n",
    "---\n",
    "\n",
    "Basically for objective $L(\\beta)+\\lambda\\sum_j|\\beta_j|$, we can rewrite it to\n",
    "$$\n",
    "L(\\beta)+\\lambda\\sum_j(\\beta_j^++\\beta_j^-) - \\sum_j\\lambda_j^+\\beta_j^+-\\sum_j\\lambda_j^-\\beta_k^-\n",
    "$$\n",
    "and at optimal solution, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla L(\\beta)j + \\lambda - \\lambda_j^+ & = 0 \\\\\n",
    "-\\nabla L(\\beta)j + \\lambda - \\lambda_j^- & = 0 \\\\\n",
    "\\lambda_j^+\\beta_j^+ & = 0 \\\\\n",
    "\\lambda_j^-\\beta_j^- & = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
